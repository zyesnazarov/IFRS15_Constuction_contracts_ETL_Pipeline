# ğŸ“Š IFRS 15 Revenue Recognition Data Pipeline (Deloitte Consulting)

## ğŸ“Œ Project Overview
This project was developed for **Deloitte Consulting** to automate the **daily calculation of recognized revenue and profit** according to **IFRS 15**. The solution is built using **Databricks, PySpark, and Delta Lake** and processes contract, transaction, and employee cost data.

## ğŸ  Project Architecture
The project consists of **three data processing layers**:

1ï¸âƒ£ **Bronze Layer (Raw Data Ingestion)**
   - Reads raw **CSV files** from Azure Data Lake (ADLS).
   - Converts data to **Parquet format** for optimized storage.
   - Moves processed files to the "processed" zone in ADLS.

2ï¸âƒ£ **Silver Layer (Data Cleaning & Transformation)**
   - Cleans and standardizes data (**removes duplicates, renames columns**).
   - Converts date formats and ensures correct data types.
   - Loads cleaned data into **Delta Tables** for further processing.

3ï¸âƒ£ **Gold Layer (IFRS 15 Revenue Recognition)**
   - Aggregates contract, transaction, and employee cost data.
   - **Calculates recognized revenue & profit** using the **percentage of completion method**.
   - Exports daily reports to **Azure Blob Storage** in **Excel format**.

## Data Processing Workflow
1. **Contracts, Transactions, and Employee Costs** are loaded from **Azure Blob Storage (CSV format)**.
2. Data is **cleaned, deduplicated, and standardized** before being stored in **Delta Tables**.
3. The **Gold Layer** performs **financial calculations**:
   - ğŸ“Œ **Cost Incurred Percentage** = (Recognized Costs + Payroll Expenses) / Estimated Costs
   - ğŸ“Œ **Recognized Revenue** = Contract Amount Ã— Cost Incurred Percentage
   - ğŸ“Œ **Recognized Profit** = Recognized Revenue - Total Costs
4. The final results are **saved in Delta Tables and exported as daily reports**.

## Technologies Used
- **ğŸ”¹ PySpark** â†’ Distributed data processing
- **ğŸ”¹ Databricks** â†’ Cloud-based analytics platform
- **ğŸ”¹ Delta Lake** â†’ Optimized storage & versioning
- **ğŸ”¹ Azure Data Lake Storage (ADLS)** â†’ Cloud storage
- **ğŸ”¹ Pandas & OpenPyxl** â†’ Excel report generation

## How to Run the Project
1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/ifrs15-revenue-pipeline.git
   ```
2. **Upload the scripts to Databricks**.
3. **Run the Bronze Layer notebook** to ingest raw data.
4. **Run the Silver Layer notebook** to clean and transform the data.
5. **Run the Gold Layer notebook** to calculate revenue and generate reports.

## ğŸ“‚ Project Files
| File | Description |
|------|------------|
| `Bronze_layer.py` | Ingests raw CSV data from Azure, converts to Parquet, and moves files. |
| `Silver_layer.py` | Cleans, deduplicates, and standardizes data before loading into Delta Tables. |
| `Gold_layer.py` | Calculates recognized revenue & profit, then exports reports to Azure. |

## ğŸ“Œ Future Enhancements
ğŸ”¹ Implement **Apache Airflow** for **pipeline automation**.  
ğŸ”¹ Optimize **Spark performance** using **Z-Ordering & Caching**.  
ğŸ”¹ Deploy an **API endpoint** to query recognized revenue in real-time.


